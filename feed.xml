<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>   
    <title>near-perfect clarity</title>
    <atom:link href="http://knownknown.github.io/feed.xml" rel="self" type="application/rss+xml"></atom:link>
    <link>http://knownknown.github.io</link>
    <description>there will be near-perfect clarity</description>
    <pubDate>Wed, 30 Apr 2014 20:00:00 -0400</pubDate>
    <generator>The mighty Wintersmith</generator>
    <language>en</language>
    <item>
      <title>Classification</title>
      <link>http://knownknown.github.io/articles/04_classified/</link>
      <pubDate>Wed, 30 Apr 2014 20:00:00 -0400</pubDate>
      <guid isPermaLink="true">http://knownknown.github.io/articles/04_classified/</guid>
      <author></author>
      <!-- passing locals.url resolves all relative urls to absolute-->
      <description>&lt;p&gt;&lt;em&gt;Meta information on the diplomat classifying each cable reveals many significant dipomat-to-entity associations. Analyzed as a graph, these associations provide a comprehensive view of major agency priorities and their overlap.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/p&gt;
&lt;p&gt;One of the conveniences of this data is that it follows a structured form with several important pieces of meta-information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Send date (which we’ve looked at previously)&lt;/li&gt;
&lt;li&gt;Subject.&lt;/li&gt;
&lt;li&gt;Origin and destination (surprisingly unimportant, with cables typically originating from a US Mission in the host country and destined for general federal departments)&lt;/li&gt;
&lt;li&gt;TAGS (very generally describing the cable contents, e.g. “External Political Relations”, “Military and Defense Arrangements”, etc.)&lt;/li&gt;
&lt;li&gt;Classification status.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An ignored data point (as far as I am aware) has been the classification status, which includes a sentence stating the person who classified it and the formal reasons. The &lt;a href=&quot;http://en.wikipedia.org/wiki/Classified_information_in_the_United_States#Classification_categories&quot;&gt;reasons themselves&lt;/a&gt; are  general, but the diplomat setting the classification essentially takes ownership of the cable. They may not be the original author (as cables are often drafted by lower-level diplomats) but they are the last person in charge once the cable leaves it’s origin. Looking for patterns across these individuals can provide insights into the structure of embassy work and the distribution of labour. And unlike assumption-free analysis of the text, these patterns represent priorities set by the department &lt;em&gt;itself&lt;/em&gt;. Instead of guessing what content was deemed important, we can learn these priorities directly. Which entities are most discussed by individuals within the department? Do they tend to work together or focus on topics individually? Which topics had focus and when? With the classification data, these questions can now be tested directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Individual associations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Perhaps one of the oldest statistical &lt;a href=&quot;http://en.wikipedia.org/wiki/Student%27s_t-test&quot;&gt;tests&lt;/a&gt; evaluates differences between two sets of data, or an &lt;em&gt;association&lt;/em&gt; between an indicator and an outcome. We can apply such techniques to this data in looking for an association between classifiers and specific topics. As with any fundamental question, there are dozens of ways to answer it, but to start I simply counted the fraction of times an entity (say “Bosnia”) is mentioned by a specific classifier and compared it to the fraction in the rest of the classified data. If the classifier mentioned Bosnia much more or less than everyone else, we can assess the significance of this deviation. Since we’re working with small count data, the Fisher’s Exact Test is appropriate in evaluating significance. This way, every classifier was tested against every entity they classified, and the final outcome was &lt;a href=&quot;http://en.wikipedia.org/wiki/Bonferroni_correction&quot;&gt;corrected&lt;/a&gt; for all the tests performed to make sure we weren’t just pulling out random patterns because we rolled the dice so many times.&lt;/p&gt;
&lt;p&gt;Turns out the data is highly structured, and the top ten classifiers (which had &amp;gt;100 cables each) all strongly associated with many terms. Below is the set of significant associations for the LOCATION category:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.5.png&quot; alt=&quot;LOCATION&quot;&gt; &lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.6.png&quot; alt=&quot;LOCATION&quot;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.10.png&quot; alt=&quot;LOCATION&quot;&gt; &lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.8.png&quot; alt=&quot;LOCATION&quot;&gt;   &lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.3.png&quot; alt=&quot;LOCATION&quot;&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.2.png&quot; alt=&quot;LOCATION&quot;&gt; &lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.1.png&quot; alt=&quot;LOCATION&quot;&gt; &lt;img src=&quot;http://knownknown.github.io/articles/04_classified/assoc.LOCATION.4.png&quot; alt=&quot;LOCATION&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The red bar corresponds to the fraction of mentions received by that entity, and the black bar is the fraction in the rest of the data. These associations are not just significant, but are also very strong in absolute terms; almost every diplomat has a topic that they discuss &lt;em&gt;much&lt;/em&gt; more than the rest. Looking for relationships between the entities, we can guess at some broad topics: Charles English &amp;amp; Judith Cefkin focus on Bosnia, Serbia, and Yugoslavia; Jerry Taylor on Ukraine and Belarus; Eric Schultz on the south-stream pipeline route spearheaded by Gazprom, with construction starting in Krasnoyarsk; etc.&lt;/p&gt;
&lt;p&gt;The overall departmental structure appears to be fairly divided, with one or two individuals focusing on each of these topics. Looking at all the topics together also fleshes out the general shape of the Russian conversation. Topics that we had previously guessed as important - Georgia, Bosnia - are now confirmed as important for specific high-level individuals, and others - such as Russian oil resources - are now on the radar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From associations to relationships&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Going beyond the individual diplomat-entity associations, we can try to visualize all of these relationships together by placing them in a graph (you knew this was coming, right? At least it’s not one of those animated, spring-loaded nightmares. But be sure to zoom in for detail):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/04_classified/graph.svg&quot; alt=&quot;graph&quot;&gt;&lt;/p&gt;
&lt;p&gt;So what’s going on here? Each node represents an entity (PERSON, LOCATION, or ORGANIZATION), and every pair of nodes has an edge weight corresponding to the number of cables they both appear in. Nodes that never appear together have a weight of zero, and so on. To preserve our sanity, only those nodes that were significantly positively associated with at least one diplomat have been plotted here, and their colours correspond to the most associated diplomat. This is expected to create a distortion in the graph, since we’re selecting precisely those entities that are polarizing, but that’s actually &lt;em&gt;desireable&lt;/em&gt;. We’re specifically interested in identifying strata which correlate with individual diplomats, and so letting the associated nodes drive the graph structure encourages this. Nodes are also scaled according to their PageRank, which basically measures how many other large nodes each node is connected to. Finally, after everything is laid out the edges are hidden for clarity (as is often the case, they’re &lt;a href=&quot;http://knownknown.github.io/articles/04_classified/graph.edges.png&quot;&gt;pretty&lt;/a&gt; but mostly meaningless; the one useful observation is that the neighbouring Gottemoeller and English nodes are mostly independent). I’ll add a caveat that many of the attractive qualities (the spherical shape, the spacing between nodes) are an artefact of the layout algorithm, so one should be careful not to draw conclusions like “&lt;em&gt;diplomatic communications are naturally geometrically circular&lt;/em&gt;“ or some-such.&lt;/p&gt;
&lt;p&gt;That said, the final network is highly informative and crystallizes our previous term-based guesses. A small number of primary groups are clearly visible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rose Gottemoeller: New START; Geneva (the site of the negotiations); SLBMS (ballistic missiles); NPT (non-proliferation treaty); and various Russian and US negotiators.&lt;/li&gt;
&lt;li&gt;Charles English: Serb/Bosnian relations; Miloard Dodik, the president of Republika Srpska; SDA (The Bosnian Party of Democratic Action); and Srebrenica. There is also strong overlap with diplomat Judith Cefkin.&lt;/li&gt;
&lt;li&gt;Jerry Taylor: Ukraine, Kazakhstan, and Belarus; apparently strong overlap with the Gottemoeller/New START work (through the JCIC - Joint Compliance and Inspection Commission).&lt;/li&gt;
&lt;li&gt;Eric Schultz: finance and natural resources, focusing on Gazprom and Lukoil.&lt;/li&gt;
&lt;li&gt;John Beyrle &amp;amp; Alice Wells: the Georgian war. With Wells focusing on Russian media (the newspapers Kommersant, Novaya Gazeta, Nezavisimaya Gazeta); reform movements (Kasparov, Nemtsov, the progressive Yabloko and Union of Right Forces parties) and hard-liners (far-right LDPR, communist KPRF parties).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we suspected most, cables focus on topics related to New START negotiations, the Orange Revolution in Ukraine, Bosnian statehood, and the Georgian war; and each topic is helmed by one or two diplomats. Now that the overall layout is understandable to us, we can add in the time dimension and see how nodes have changed through the 2006-2010 period. The &lt;a href=&quot;http://knownknown.github.io/articles/04_classified/time_graph.gif&quot;&gt;animated graph&lt;/a&gt; (warning, large-ish file) reveals additional trends: a persistent focus on the Bosnian issue; the late-2009/early-2010 focus on New START and missile defense; near-total focus on the Georgian war in 08/2008, with little fore-warning; an initial focus on natural resources and Moscow politics that nearly disappears by 2010.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph or trap?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’ll admit that I’ve dreaded looking at the data in this massive graph form. Such visualizations tend to be extremely attractive and carry the &lt;em&gt;appearence&lt;/em&gt; of information, but often confuse more than they inform. Usually, the viewer is awed by the complexity of the data and then moves on with their day. Perhaps they see a few clusters or over-represented colours and think “&lt;em&gt;This data sure is complex. But it’s also structured.&lt;/em&gt;“, having learned no more than what could be presented by a handful of statistics. That may be even worse.&lt;/p&gt;
&lt;p&gt;To make sure we’re not just being seduced by &lt;a href=&quot;http://eagereyes.org/techniques/graphs-hairball&quot;&gt;hairballs&lt;/a&gt; it’s useful to think about how much more information we’ve &lt;em&gt;really&lt;/em&gt; gained over looking at the data in sorted tables. The main contribution of the graph is in summarizing the proximity of terms. For example, many seemingly general terms  - elliott, smith, gross, dean - now come out as corresponding to the New START negotiations. Distance is also revealed: we see that Diplomat Wells focuses both on topics relating to the Georgia war (sakkashvili, tskhinvali) as well as unrelated issues such as Russian newspapers. Lastly, we can get a sense of the complexity of each topic: the Georgian issue is driven by a handful of major entities; while New START includes many equally represented individuals and concepts; and Bosnian statehood is somewhere in between.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Time Series</title>
      <link>http://knownknown.github.io/articles/03_time_series/</link>
      <pubDate>Mon, 28 Apr 2014 20:00:00 -0400</pubDate>
      <guid isPermaLink="true">http://knownknown.github.io/articles/03_time_series/</guid>
      <author></author>
      <!-- passing locals.url resolves all relative urls to absolute-->
      <description>&lt;p&gt;&lt;em&gt;Examining the distribution of cables and entities over time provides important context and identifies a small number of well-documented entities that are likely to be important. However, we’re still not capturing the direct relationships between these entities.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/p&gt;
&lt;p&gt;In the previous post, a naive list of highly occurring entities provided some sense of the diversity of this data as well as a condensed “Top 10” of the people, locations, and organizations discussed in the cables. One dimension that’s entirely missing from that approach is that of time. Each cable has been coded with a time-stamp for when it was sent, allowing us to look at the frequency of communication across the available time-frame. To get a birds eye view of all cables, I’ve plotted their number over consecutive 10-day periods, with the year breaks indicated:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/03_time_series/time_ser.total.png&quot; alt=&quot;time-series total&quot;&gt;&lt;/p&gt;
&lt;p&gt;It’s immediately obvious that the bulk of the cables were gathered from 2006-2010. We can similarly zoom in on cables mentioning specific entities and examine their time series. As an example, let’s take a look at occurrences over time for two of the previously identified common words “Saakashvili” (the president of Georgia) and “South Ossetia” (the disputed, formerly Georgian, territory). The representation is the same as before, but I’ve also highlighted the explicit start of the Russo-Georgian War in August, 2008:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/03_time_series/time_ser.example.png&quot; alt=&quot;time-series South Ossetia example&quot;&gt;&lt;/p&gt;
&lt;p&gt;Lo and behold, we see a substantial up-tick in mentions of both entities at the start of the war. This visualization makes it clear that the great number of Saakashvili mentions is almost entirely explained by events of late 2008. We can also see that both entities were mentioned regularly but infrequently in the preceding years, and that the increase in mentions subsided almost entirely by the start of 2009, particularly for Saakashvili. That the spike in mentions happened in the same week as the deceleration of war (even though tensions between Georgia and Russia had been high since at least April) suggests that US diplomats were not unusually focused on the issue prior to military escalation. Just looking at these occurrences over time has provided a great deal of context.&lt;/p&gt;
&lt;p&gt;Similarly, we can look at common entity occurrence surrounding the March, 2008 election of pint-sized president Dimitri Medvedev:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/03_time_series/time_ser.example2.png&quot; alt=&quot;time-series Medvedev example&quot;&gt;&lt;/p&gt;
&lt;p&gt;In this instance, the increase in mentions of Medvedev preceded his election by months (starting around the time of his nomination in January, 2008), indicative of some advanced knowledge. On the other hand, the nearly complete lack of mentions prior to 2008 suggests that he had not recently been a key diplomatic figure. In comparison, Putin is discussed consistently throughout the entire analysed period and his stature is largely unaffected after being replaced by Medvedev.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Broader trends&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking at a small number of entities is clearly informative, but somewhat unrealistic; in the above examples we started with a known event and looked at it in relation to certain known entities. What we actually want is the complete opposite: start with a pile of unknown data and have the analyses reveal both the important entities &lt;em&gt;and&lt;/em&gt; the events they are linked with. This turns out to be more difficult that it first seemed. If we simply look at the most mentioned entities in each month, we tend to see individuals that appear consistently throughout the time-line or their administration. As in the Putin example above, discussion of these individuals (Medvedev, Lavrov, Obama, etc.) doesn’t vary much over time, and so the time-dimension becomes uninformative. At best, we’ll know when someone big has come into office; at worst, we’ll just have a messier version of the “Top 10” list. We can deal with this by normalizing each entity by their total number of occurrences and then look for monthly peaks, but that shifts the data too much in the direction of minorities. After normalization, someone who gets 1,000 mentions in Jan. and Feb. ranks below someone who gets 2 mentions in Jan. only, so individual blips get hugely inflated.&lt;/p&gt;
&lt;p&gt;The working solution is to use a hybrid of the two strategies: first the blips are weeded out, then the remaining important entities are normalized and compared. For every month, we identify all entities that are the most mentioned - these are important enough to show up frequently &lt;em&gt;at some point in time&lt;/em&gt;.  We can then tune the monthly inclusion threshold to select the desired number of entities in total. After selecting this set of core individuals we normalize them and examine their occurrences over time. The approach is &lt;em&gt;ad hoc&lt;/em&gt; in the inclusion, but that will primarily effect the overall number of samples viewed.&lt;/p&gt;
&lt;p&gt;Below is the horizon plot for LOCATION entities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/03_time_series/time_ser.LOCATION.png&quot; alt=&quot;LOCATION distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;Fundamentally, this is not much different from a heat-map or “small-multiples” chart, but it provides more granularity than the former while still being very efficient. One concern is that the &lt;em&gt;ordering&lt;/em&gt; of individual rows can affect the interpretation, and here they have been grouped using simple hierarchical clustering. This tends to place similar trend-lines together so that correlations are easier to pick out by eye (though it is far from perfect). Continuing from the previous example, we see that “South Ossetia”, “Abkhazia”, and “Georgia” all cluster together and are driven by a peak around the August ‘08 War.&lt;/p&gt;
&lt;p&gt;We can similarly examine the time series of top PERSON entities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/03_time_series/time_ser.PERSON.png&quot; alt=&quot;PERSON distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;Several general time-related topics are starting to emerge. We can clearly see the changing administrations in 2009 as the mentions of Secretary Rice are replaced by Barack Obama, Hilary Clinton, and Rose Gottemoeller (Assistant Secretary of State for the Bureau of Arms Control, Verification and Compliance). We also see a 2007 cluster of Bosnian politicians (Ivanic, Tihic, Silajdzic) likely related with Ahtisaari, the UN envoy for the Kosovo status process. Also interesting is the consistent presence of Milorad Dodik. Dodik was one of the surprising entities at the top of all cables, and it’s clear now that this was not a fluke or due to a single key date.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking at the data across time also makes clear it that we’re still missing &lt;em&gt;a coherent, time-coded context for each of the well-represented entities&lt;/em&gt;. Many context-specific questions remain unanswered: What is being discussed in the flurry of cables on Milorad Dodik at the end of 2008? Are the punctuated entities (e.g. Elisabeth Millard and Stephen D Krasner) co-occurring by chance or do they have a relationship? In short, does each of these rows represent an individual event or just one strand from a complex, correlated community? We’ve gained some intuition by looking at occurrences over time, but these questions can only be answered by broadening our analysis to &lt;em&gt;co-occurrences&lt;/em&gt; of entities within cables and across time.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>A Brief Note on Sentiment</title>
      <link>http://knownknown.github.io/articles/02_sentiment/</link>
      <pubDate>Tue, 22 Apr 2014 20:00:00 -0400</pubDate>
      <guid isPermaLink="true">http://knownknown.github.io/articles/02_sentiment/</guid>
      <author></author>
      <!-- passing locals.url resolves all relative urls to absolute-->
      <description>&lt;p&gt;&lt;em&gt;Inferring the emotional content of text opens up very cool possibilities, but it’s just not appropriate for data with this level of nuance and complexity.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the more exciting NLP applications to this data is “sentiment” analysis, which aims to automatically classify sentences and entities with the writer’s contextual emotions. Naively, one could this by simply taking a set of words with known sentiment scores (“happy”=positive, “awful”=negative, etc.) and counting up their occurrences to get an overall sentence or document score (the so-called &lt;em&gt;bag of words&lt;/em&gt; model which ignores word order and grammar). This has some obvious draw-backs (“he was happy to spit in our face”, “we had an awfully good time”) and has been heavily extended to take into account local and even multi-sentence context. The Stanford &lt;a href=&quot;http://nlp.stanford.edu/sentiment/&quot;&gt;Recursive Model&lt;/a&gt;, for example, turns the sentence into a tree using parts-of-speech, then builds a combined score from the leaves up. Perhaps “awfully good” could be correctly understood as single positive phrase instead of one positive word and one negative word. If the algorithm works, the implications can be tantalizing; see, for example, the IKANOW &lt;a href=&quot;http://www.ikanow.com/making-the-most-of-sentiment-scores-with-ikanow-and-r/&quot;&gt;analysis&lt;/a&gt; of Enron e-mails; and Saif Mohammad’s time-series &lt;a href=&quot;http://www.saifmohammad.com/WebDocs/NRC-TechReport-emotions-in-books-and-mail.pdf&quot;&gt;analysis&lt;/a&gt; of sentiment in books. In the context of diplomatic data, quantifying sentiment would express in numbers the cloud of emotions surrounding those actors and organizations we’re studying:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What individuals are the most/least liked?&lt;/li&gt;
&lt;li&gt;How has that changed over time?&lt;/li&gt;
&lt;li&gt;Which are the most controversial, liked and hated in equal measure?&lt;/li&gt;
&lt;li&gt;Which diplomats are unusually friendly to some individuals but not others? This last one is especially important as it provides insight into the potential biases of our narrators.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This emotional content may be even more informative than the factual, especially when the data has been heavily ascertained from one point-of-view. As it happens, bag-of-words sentiment analysis has been applied to such data to quantify U.S. relationships with foreign nations (&lt;a href=&quot;http://www.technologyreview.com/view/423601/automated-processing-of-wikileaks-cables-reveals-us-friends-foes/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://vikparuchuri.com/blog/tracking-us-sentiments-over-time-in/&quot;&gt;here&lt;/a&gt;). Those projects are novel and the application clever, but the results are clearly noisy and not particularly informative without many assumptions. Perhaps we could do better by using a more advanced sentiment parser; analysing a single region where word-choice is more likely to be controlled; linking the sentiment to known political actors, and looking at a generally richer dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s start with the good news. Here are some example sentences that had the highest sentiment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analysts predict several decades of sustained robust economic growth, thanks in part to India’s youthful population and its technical and scientific prowess.&lt;/li&gt;
&lt;li&gt;USAID post-tsunami reconstruction projects are moving ahead smartly.&lt;/li&gt;
&lt;li&gt;He has good Kremlin access and a well-developed sense of what is realistic in light of Kremlin policies.&lt;/li&gt;
&lt;li&gt;He and President Putin are very popular and receive credit for Belgorod’s strong economy, active civil society, and vibrant academia.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the second instance we almost certainly got lucky that some other British-ism hadn’t been used instead of “smartly” (say, “briskly”). And the last two examples are nuanced in the real subject of their approval. But let’s not quibble, these are really quite positive!&lt;/p&gt;
&lt;p&gt;The bad news, though, is that these examples are pretty much the only true positives (in every sense of the word) over the tens of thousands of sentences that received sentiment scores before my computer said “enough”. Nearly every sentence was given a negative score, and examining individual cables that had extreme sentiment averages showed no indication of accuracy. If anything, the average sentiment was mostly representative of cable size and complexity (which makes sense, as the sentiment parser appears to down-weight complex sentences with positive words). None of the most negatively ranked cables I looked at could reasonably be interpreted as accurately classified. This, coupled with the fact that sentiment analysis is easily the most computationally intensive part of the language analysis, lead me to reluctantly abandon the strategy.&lt;/p&gt;
&lt;p&gt;Sentiment analysis clearly has value, and sentiment-based predictors of everything from &lt;a href=&quot;http://www.lct-master.org/files/MullenSentimentCourseSlides.pdf&quot;&gt;music reviews to twitter messages&lt;/a&gt; already do very well. But pulling out emotional terms from 140 characters appears to be a much easier problem that annotating large, nuanced political documents.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Getting Started - Named Entities</title>
      <link>http://knownknown.github.io/articles/01_named_entities/</link>
      <pubDate>Mon, 21 Apr 2014 20:00:00 -0400</pubDate>
      <guid isPermaLink="true">http://knownknown.github.io/articles/01_named_entities/</guid>
      <author></author>
      <!-- passing locals.url resolves all relative urls to absolute-->
      <description>&lt;p&gt;&lt;em&gt;Extracting important entities from the text is a useful first step and provides a birds-eye view of all the content. But what we’re interested in is most likely at the margins.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The entire Cablegate release consists of over 250,000 cables from a 20 year period, over a gigabyte of full-text storage. To get a sense of the data, these initial analyses will focus on the 8,000 cables tagged “RS”, corresponding to communication about Russia and neighbouring countries. Russia was chosen for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It’s one of the single most-mentioned countries outside of the Middle East. At the same time, it appears to be strongly under-reported, as news agencies focused on cables relating to Iraq and Afghanistan or colourful personalities.&lt;/li&gt;
&lt;li&gt;It’s an extremely important American adversary, with several major geo-political events (such as the Georgian War) occurring during the leaked time-frame. As a consequence, it features key players and organizations that are mostly well-known in the West.&lt;/li&gt;
&lt;li&gt;It’s continued to engage in military activity that strongly mirrors the events of the Georgian War, and so there is hope that diplomatic communications from that time will inform our understanding of current events.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first step in analysing so much text is breaking it down into major concepts and establishing their relationships. Concepts can be difficult to pin down, but the automated extraction of specific objects has been well-developed in the field of Natural Language Processing (NLP). Termed Named Entity Recognition (NER), such methods use sentence context and parts-of-speech to identify specific categories of words (typically nouns) based on a training set. For this data, I applied the &lt;a href=&quot;http://nlp.stanford.edu/software/CRF-NER.shtml&quot;&gt;Stanford NER&lt;/a&gt; to extract PERSON, LOCATION, and ORGANIZATION entities based on a training set of newspaper text. We’re lucky that most of the cables are written in newspaper style, so the training set is a good match. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Top entities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Russia/RS cables tagged consist of 7,803 locations; 19,105 organizations; and 20,680 persons. Roughly a quarter of the entities appear in at least two cables, and I’ll focus on these for now as they are less likely to be false identifications. A total of 280 (2%) unique entities account for 50% of the entity-to-cable relationships. The most over-represented entities are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;LOCATION&lt;/th&gt;
&lt;th&gt;ORGANIZATION&lt;/th&gt;
&lt;th&gt;PERSON&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;6.7% russia&lt;/td&gt;
&lt;td&gt;2.6% mfa&lt;/td&gt;
&lt;td&gt;2.9% putin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.5% us&lt;/td&gt;
&lt;td&gt;2.1% eu&lt;/td&gt;
&lt;td&gt;1.9% medvedev&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.5% moscow&lt;/td&gt;
&lt;td&gt;2.0% nato&lt;/td&gt;
&lt;td&gt;1.1% lavrov&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.0% united states&lt;/td&gt;
&lt;td&gt;1.4% un&lt;/td&gt;
&lt;td&gt;0.7% obama&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.9% georgia&lt;/td&gt;
&lt;td&gt;1.1% osce&lt;/td&gt;
&lt;td&gt;0.7% gottemoeller&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.5% washington&lt;/td&gt;
&lt;td&gt;0.8% gazprom&lt;/td&gt;
&lt;td&gt;0.6% saakashvili&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.5% eu&lt;/td&gt;
&lt;td&gt;0.6% ngo&lt;/td&gt;
&lt;td&gt;0.5% milorad dodik&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.5% europe&lt;/td&gt;
&lt;td&gt;0.6% duma&lt;/td&gt;
&lt;td&gt;0.4% bush&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.3% ukraine&lt;/td&gt;
&lt;td&gt;0.6% usg&lt;/td&gt;
&lt;td&gt;0.3% dodik&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.2% china&lt;/td&gt;
&lt;td&gt;0.5% unsc&lt;/td&gt;
&lt;td&gt;0.3% taylor&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Another way of representing this is to plot the cumulative distribution of occurrences ordered by their frequency. The full set of entities is too big to list, but we can reveal the entities at regular intervals to get an idea of the types of names in that part of the distribution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/01_named_entities/cumsum.PERSON.png&quot; alt=&quot;PERSON distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/01_named_entities/cumsum.LOCATION.png&quot; alt=&quot;LOCATION distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://knownknown.github.io/articles/01_named_entities/cumsum.ORGANIZATION.png&quot; alt=&quot;ORGANIZATION distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;The grey line here is what we would expect to see if the same number of entities were just mentioned uniformly by each cable (it’s also slightly convex because we’re ordering by frequency). The difference between these two lines is a rough metric of the diversity of the text. For example, we see that top locations reoccur much more frequently than top persons. In fact, only 48 locations account for 50% of &lt;em&gt;all&lt;/em&gt; instances where a location is mentioned (compared to 448 organizations, and 1115 persons).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking at the top entities, most of the locations and organizations are unsurprising and consist of common American or European terms and synonyms. The presence of Georgia and Ukraine this high on the list stands out, and is indicative of the important role those two countries play in the Russian “near-abroad”. This importance is further reflected in the abundance of references to Georgian president Saakashvili. Serbian Serbian &lt;a href=&quot;http://en.wikipedia.org/wiki/Milorad_Dodik&quot;&gt;politician&lt;/a&gt; Miloard Dodik is the only surprise and the only other foreign entity. It’s interesting to note that Dodik appears to be a continued key player in Moscow and ardent supporter of the recent Russian occupation of Crimea.&lt;/p&gt;
&lt;p&gt;One way of looking at this is that NER has reduced a set of documents containing 1.2 million lines down to the few key players with no apparent false-positives. Had we applied this approach to a secretive organization this table would already by quite useful. On the other hand, all we &lt;em&gt;really&lt;/em&gt; learned could have been gleaned by skimming through World Fact Book on the US and Russia. NER is clearly relevant, but higher-level analysis will be needed to uncover truly novel information.&lt;/p&gt;
</description>
    </item>
  </channel>
</rss>