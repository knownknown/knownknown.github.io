<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>near-perfect clarity
    </title>
    <link rel="alternate" href="http://knownknown.github.io/feed.xml" type="application/rss+xml" title="there will be near-perfect clarity">
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Alegreya:400italic,400">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Alegreya+SC">
  </head>
  <body>
    <div class="container">
      <header><img src="http://knownknown.github.io/globe.png">
        <p></p><a href="http://knownknown.github.io/index.html" class="blog-title">near-perfect clarity</a>
      </header>
      <article class="post">
        <h2 class="index-post-title"><a href="http://knownknown.github.io/articles/03_time_series/index.html">Time Series</a></h2>
        <p class="date"><span>29 April 2014</span></p>
        <section class="intro-content"><p><em>TLDR: Examining the distribution of cables and entities over time provides important context and identifies a small number of well-documented entities that are likely to be important. However, we’re still missing the direct relationships between these entities.</em></p>
<p>In the previous post, a naive list of highly occurring entities provided some sense of the diversity of this data as well as a condensed “Top 10” of the people, locations, and organizations discussed in the cables. One dimension that’s entirely missing from that approach is that of time. Each cable has been coded with a time-stamp for when it was sent, allowing us to look at the frequency of communication across the available time-frame. To get a birds eye view of all cables, I’ve plotted their number over consecutive 10-day periods, with the year breaks indicated:</p>
<p><img src="/articles/03_time_series/time_ser.total.png" alt="time-series total"></p>
<p>It’s immediately obvious that the bulk of the cables were gathered from 2006-2010. We can similarly zoom in on cables mentioning specific entities and examine their time series. As an example, let’s take a look at occurrences over time for two of the previously identified common words “Saakashvili” (the president of Georgia) and “South Ossetia” (the disputed, formerly Georgian, territory). The representation is the same as before, but I’ve also highlighted the explicit start of the Russo-Georgian War in August, 2008:</p>
<p><img src="/articles/03_time_series/time_ser.example.png" alt="time-series South Ossetia example"></p>
<p>Lo and behold, we see a substantial up-tick in mentions of both entities at the start of the war. This visualization makes it clear that the great number of Saakashvili mentions is almost entirely explained by events of late 2008. We can also see that both entities were mentioned regularly but infrequently in the preceding years, and that the increase in mentions subsided almost entirely by the start of 2009, particularly for Saakashvili. That the spike in mentions happened in the same week as the deceleration of war (even though tensions between Georgia and Russia had been high since at least April) suggests that US diplomats were not unusually focused on the issue prior to military escalation. Just looking at these occurrences over time has provided a great deal of context.</p>
<p>Similarly, we can look at common entity occurrence surrounding the March, 2008 election of pint-sized president Dimitri Medvedev:</p>
<p><img src="/articles/03_time_series/time_ser.example2.png" alt="time-series Medvedev example"></p>
<p>In this instance, the increase in mentions of Medvedev preceded his election by months (starting around the time of his nomination in January, 2008), indicative of some advanced knowledge. On the other hand, the nearly complete lack of mentions prior to 2008 suggests that he had not recently been a key diplomatic figure. In comparison, Putin is discussed consistently throughout the entire analysed period and his stature is largely unaffected after being replaced by Medvedev.</p>
<p><strong>Broader trends</strong></p>
<p>Looking at a small number of entities is clearly informative, but somewhat unrealistic; in the above examples we started with a known event and looked at it in relation to certain known entities. What we actually want is the complete opposite: start with a pile of unknown data and have the analyses reveal both the important entities <em>and</em> the events they are linked with. This turns out to be more difficult that it first seemed. If we simply look at the most mentioned entities in each month, we tend to see individuals that appear consistently throughout the time-line or their administration. As in the Putin example above, discussion of these individuals (Medvedev, Lavrov, Obama, etc.) doesn’t vary much over time, and so the time-dimension becomes uninformative. At best, we’ll know when someone big has come into office; at worst, we’ll just have a messier version of the “Top 10” list. We can deal with this by normalizing each entity by their total number of occurrences and then look for monthly peaks, but that shifts the data too much in the direction of minorities. After normalization, someone who gets 1,000 mentions in Jan. and Feb. ranks below someone who gets 2 mentions in Jan. only, so individual blips get hugely inflated.</p>
<p>The working solution is to use a hybrid of the two strategies: first the blips are weeded out, then the remaining important entities are normalized and compared. For every month, we identify all entities that are the most mentioned - these are important enough to show up frequently <em>at some point in time</em>.  We can then tune the monthly inclusion threshold to select the desired number of entities in total. After selecting this set of core individuals we normalize them and examine their occurrences over time. The approach is <em>ad hoc</em> in the inclusion, but that will primarily effect the overall number of samples viewed.</p>
<p>Below is the horizon plot for LOCATION entities:</p>
<p><img src="/articles/03_time_series/time_ser.LOCATION.png" alt="LOCATION distribution"></p>
<p>Fundamentally, this is not much different from a heat-map or “small-multiples” chart, but it provides more granularity than the former while still being very efficient. One concern is that the <em>ordering</em> of individual rows can affect the interpretation, and here they have been grouped using simple hierarchical clustering. This tends to place similar trend-lines together so that correlations are easier to pick out by eye (though it is far from perfect). Continuing from the previous example, we see that “South Ossetia”, “Abkhazia”, and “Georgia” all cluster together and are driven by a peak around the August ‘08 War.</p>
<p>We can similarly examine the time series of top PERSON entities:</p>
<p><img src="/articles/03_time_series/time_ser.PERSON.png" alt="PERSON distribution"></p>
<p>Several general time-related topics are starting to emerge. We can clearly see the changing administrations in 2009 as the mentions of Secretary Rice are replaced by Barack Obama, Hilary Clinton, and Rose Gottemoeller (Assistant Secretary of State for the Bureau of Arms Control, Verification and Compliance). We also see a 2007 cluster of Bosnian politicians (Ivanic, Tihic, Silajdzic) likely related with Ahtisaari, the UN envoy for the Kosovo status process. Also interesting is the consistent presence of Milorad Dodik. Dodik was one of the surprising entities at the top of all cables, and it’s clear now that this was not a fluke or due to a single key date.</p>
<p><strong>Discussion</strong></p>
<p>Looking at the data across time also makes clear it that we’re still missing <em>a coherent, time-coded context for each of the well-represented entities</em>. Many context-specific questions remain unanswered: What is being discussed in the flurry of cables on Milorad Dodik at the end of 2008? Are the punctuated entities (e.g. Elisabeth Millard and Stephen D Krasner) co-occurring by chance or do they have a relationship? In short, does each of these rows represent an individual event or just one strand from a complex, correlated community? We’ve gained some intuition by looking at occurrences over time, but these questions can only be answered by broadening our analysis to <em>co-occurrences</em> of entities within cables and across time.</p>
</section>
      </article>
      <article class="post">
        <h2 class="index-post-title"><a href="http://knownknown.github.io/articles/02_sentiment/index.html">A Brief Note on Sentiment</a></h2>
        <p class="date"><span>23 April 2014</span></p>
        <section class="intro-content"><p><em>TLDR: Inferring the emotional content of text opens up very cool possibilities, but it’s just not appropriate for data with this level of nuance and complexity.</em></p>
<p><strong>Introduction</strong></p>
<p>One of the more exciting NLP applications to this data is “sentiment” analysis, which aims to automatically classify sentences and entities with the writer’s contextual emotions. Naively, one could this by simply taking a set of words with known sentiment scores (“happy”=positive, “awful”=negative, etc.) and counting up their occurrences to get an overall sentence or document score (the so-called <em>bag of words</em> model which ignores word order and grammar). This has some obvious draw-backs (“he was happy to spit in our face”, “we had an awfully good time”) and has been heavily extended to take into account local and even multi-sentence context. The Stanford <a href="http://nlp.stanford.edu/sentiment/">Recursive Model</a>, for example, turns the sentence into a tree using parts-of-speech, then builds a combined score from the leaves up. Perhaps “awfully good” could be correctly understood as single positive phrase instead of one positive word and one negative word. If the algorithm works, the implications can be tantalizing; see, for example, the IKANOW <a href="http://www.ikanow.com/making-the-most-of-sentiment-scores-with-ikanow-and-r/">analysis</a> of Enron e-mails; and Saif Mohammad’s time-series <a href="http://www.saifmohammad.com/WebDocs/NRC-TechReport-emotions-in-books-and-mail.pdf">analysis</a> of sentiment in books. In the context of diplomatic data, quantifying sentiment would express in numbers the cloud of emotions surrounding those actors and organizations we’re studying:</p>
<ul>
<li>What individuals are the most/least liked?</li>
<li>How has that changed over time?</li>
<li>Which are the most controversial, liked and hated in equal measure?</li>
<li>Which diplomats are unusually friendly to some individuals but not others? This last one is especially important as it provides insight into the potential biases of our narrators.</li>
</ul>
<p>This emotional content may be even more informative than the factual, especially when the data has been heavily ascertained from one point-of-view. As it happens, bag-of-words sentiment analysis has been applied to such data to quantify U.S. relationships with foreign nations (<a href="http://www.technologyreview.com/view/423601/automated-processing-of-wikileaks-cables-reveals-us-friends-foes/">here</a> and <a href="http://vikparuchuri.com/blog/tracking-us-sentiments-over-time-in/">here</a>). Those projects are novel and the application clever, but the results are clearly noisy and not particularly informative without many assumptions. Perhaps we could do better by using a more advanced sentiment parser; analysing a single region where word-choice is more likely to be controlled; linking the sentiment to known political actors, and looking at a generally richer dataset.</p>
<p><strong>Results</strong></p>
<p>Let’s start with the good news. Here are some example sentences that had the highest sentiment:</p>
<ul>
<li>Analysts predict several decades of sustained robust economic growth, thanks in part to India’s youthful population and its technical and scientific prowess.</li>
<li>USAID post-tsunami reconstruction projects are moving ahead smartly.</li>
<li>He has good Kremlin access and a well-developed sense of what is realistic in light of Kremlin policies.</li>
<li>He and President Putin are very popular and receive credit for Belgorod’s strong economy, active civil society, and vibrant academia.</li>
</ul>
<p>In the second instance we almost certainly got lucky that some other British-ism hadn’t been used instead of “smartly” (say, “briskly”). And the last two examples are nuanced in the real subject of their approval. But let’s not quibble, these are really quite positive!</p>
<p>The bad news, though, is that these examples are pretty much the only true positives (in every sense of the word) over the tens of thousands of sentences that received sentiment scores before my computer said “enough”. Nearly every sentence was given a negative score, and examining individual cables that had extreme sentiment averages showed no indication of accuracy. If anything, the average sentiment was mostly representative of cable size and complexity (which makes sense, as the sentiment parser appears to down-weight complex sentences with positive words). None of the most negatively ranked cables I looked at could reasonably be interpreted as accurately classified. This, coupled with the fact that sentiment analysis is easily the most computationally intensive part of the language analysis, lead me to reluctantly abandon the strategy.</p>
<p>Sentiment analysis clearly has value, and sentiment-based predictors of everything from <a href="http://www.lct-master.org/files/MullenSentimentCourseSlides.pdf">music reviews to twitter messages</a> already do very well. But pulling out emotional terms from 140 characters appears to be a much easier problem that annotating large, nuanced political documents.</p>
</section>
      </article>
      <article class="post">
        <h2 class="index-post-title"><a href="http://knownknown.github.io/articles/01_named_entities/index.html">Named Entities</a></h2>
        <p class="date"><span>22 April 2014</span></p>
        <section class="intro-content"><p><em>TLDR: Extracting important entities from the text is an important first step and provides a birds-eye view of all the content. But what we’re interested in is most likely at the margins.</em></p>
<p><strong>Introduction</strong></p>
<p><strong>Results</strong></p>
<p>The cables tagged “RS” consist of 7,803 locations; 19,105 organizations; and 20,680 persons. Roughly a quarter of the entities appear in at least two cables, and I’ll focus on these for now as they are less likely to be false identifications. A total of 280 (2%) unique entities account for 50% of the entity-to-cable relationships. The most over-represented entities are:</p>
<table>
<thead>
<tr>
<th>LOCATION</th>
<th>ORGANIZATION</th>
<th>PERSON</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.7% russia</td>
<td>2.6% mfa</td>
<td>2.9% putin</td>
</tr>
<tr>
<td>5.5% us</td>
<td>2.1% eu</td>
<td>1.9% medvedev</td>
</tr>
<tr>
<td>4.5% moscow</td>
<td>2.0% nato</td>
<td>1.1% lavrov</td>
</tr>
<tr>
<td>2.0% united states</td>
<td>1.4% un</td>
<td>0.7% obama</td>
</tr>
<tr>
<td>1.9% georgia</td>
<td>1.1% osce</td>
<td>0.7% gottemoeller</td>
</tr>
<tr>
<td>1.5% washington</td>
<td>0.8% gazprom</td>
<td>0.6% saakashvili</td>
</tr>
<tr>
<td>1.5% eu</td>
<td>0.6% ngo</td>
<td>0.5% milorad dodik</td>
</tr>
<tr>
<td>1.5% europe</td>
<td>0.6% duma</td>
<td>0.4% bush</td>
</tr>
<tr>
<td>1.3% ukraine</td>
<td>0.6% usg</td>
<td>0.3% dodik</td>
</tr>
<tr>
<td>1.2% china</td>
<td>0.5% unsc</td>
<td>0.3% taylor</td>
</tr>
</tbody>
</table>
<p>Another way of representing this is to plot the cumulative distribution of occurrences ordered by their frequency. The full set of entities is too big to list, but we can reveal the entities at regular intervals to get an idea of the types of names in that part of the distribution.</p>
<p><img src="/articles/01_named_entities/cumsum.PERSON.png" alt="PERSON distribution"></p>
<p><img src="/articles/01_named_entities/cumsum.LOCATION.png" alt="LOCATION distribution"></p>
<p><img src="/articles/01_named_entities/cumsum.ORGANIZATION.png" alt="ORGANIZATION distribution"></p>
<p>The grey line here is what we would expect to see if the same number of entities were just mentioned uniformly by each cable (it’s also slightly convex because we’re ordering by frequency). The difference between these two lines is a rough metric of the diversity of the text. For example, we see that top locations reoccur much more frequently than top persons. In fact, only 48 locations account for 50% of <em>all</em> instances where a location is mentioned (compared to 448 organizations, and 1115 persons).</p>
<p><strong>Discussion</strong></p>
<p>Looking at the top entities, most of the locations and organizations are unsurprising and consist of common American or European terms and synonyms. The presence of Georgia and Ukraine this high on the list stands out, and is indicative of the important role those two countries play in the Russian “near-abroad”. This importance is further reflected in the abundance of references to Georgian president Saakashvili. Serbian Serbian <a href="http://en.wikipedia.org/wiki/Milorad_Dodik">politician</a> Miloard Dodik is the only surprise and the only other foreign entity. It’s interesting to note that Dodik appears to be a continued key player in Moscow and ardent supporter of the recent Russian occupation of Crimea.</p>
<p>One of way of looking at this is that NER has reduced a set of documents containing 1.2 million lines down to the few key players with no apparent false-positives. Had we applied this approach to a secretive organization this table would already by quite useful. On the other hand, all we <em>really</em> learned could have been gleaned by skimming through World Fact Book on the US and Russia. NER is clearly relevant, but higher-level analysis will be needed to uncover truly novel information.</p>
</section>
      </article>
    </div>
    <div class="footer-container">
      <div class="container">
        <footer>
          <ul>
            <li><a href="http://knownknown.github.io/index.html">Home</a></li>
            <li><a href="http://knownknown.github.io/about.html">About</a></li>
            <li><a href="http://knownknown.github.io/archive.html">Archive</a></li>
          </ul>
          <p class="copyright">&copy;2014 Author</p>
        </footer>
      </div>
    </div>
  </body>
</html>